# EXAM++  (exampp): EXAM Answerability Metric with generative LLMs


##  Installation using nix

1. install `nix` <https://nix.dev/install-nix>
2. Clone this repository and cd into it
3. in a shell type:  `nix develop`


If you are getting error message about unfree packages or experimental command, then run one of these longer commands instead

* `nix --extra-experimental-features 'nix-command flakes' develop` od 
* `NIXPKGS_ALLOW_UNFREE=1 nix --extra-experimental-features 'nix-command flakes' develop --impure`


## Installation via setup.py

0. optional: Install `trec_eval`

1. Clone this repository and cd into it
2. call `python setup.py install`

## Alternative Installation

Basic (untested) project definitions are available for `poetry`, `pipenv`, and `pip`. 

## Usage

The code works in three phases

1. **Input Preparation Phase**:  
   1. Convert your input data into a list of `exam_pp.parse_qrels_runs_with_text.QueryWithFullParagraphList` objects (one per query). 
   2. write those to file (common filepattern "xxx.json.gz") using this function
   ```
   exam_pp.parse_qrels_runs_with_text.writeQueryWithFullParagraphs(file_path:Path, queryWithFullParagraphList:List[QueryWithFullParagraphList])
   ``` 


2. **Grading Phase**:
   1. call `python -m exam_pp.exam_grading` and follow the help (`-h`) to obtain EXAM grades for all passages in your input file

3. **Evaluation and Post Pocessing Phase**: (one of two options)
   1. to obtain just the EXAM Cover evaluation score, call `python -m exam_pp.exam_cover_metric` and following the help (`-h`)
   2. to run all kinds of post-hoc analysis (obtain leaderboards, qrels, and study correlation patterns) call `python -m exam_pp.exam_postpipeline` and follow the help (`-h`) 
   this requires to have `trec_eval` to be available in your path.


Optionally, you can directly obtain EXAM Cover evaluation metric scores by loading graded inputs via


```python
exam_cover_metric.compute_exam_cover_scores(query_paragraphs:List[QueryWithFullParagraphList], exam_factory: ExamCoverScorerFactory, rank_cut_off:int=20)-> Dict[str, exam_cover_metric.ExamCoverEvals]
```


Usage Example for the Evaluation Phase from `example.py``:  

```python
from pathlib import Path
from typing import Dict, List
from exam_pp.exam_cover_metric import compute_exam_cover_scores, write_exam_results, ExamCoverScorerFactory
from exam_pp.parse_qrels_runs_with_text import GradeFilter, QueryWithFullParagraphList, parseQueryWithFullParagraphs

# Read Graded Exam files, compute EXAM Cover Evaluation Metric
query_paragraphs:List[QueryWithFullParagraphList] = parseQueryWithFullParagraphs(Path("exam-graded.jsonl.gz"))
exam_factory = ExamCoverScorerFactory(GradeFilter.noFilter(), min_self_rating=None)
resultsPerMethod = compute_exam_cover_scores(query_paragraphs, exam_factory=exam_factory, rank_cut_off=20)


# Print Exam Cover Evaluation Scores
for examEval in resultsPerMethod.values():
    print(f'{examEval.method}  exam={examEval.examScore:0.2f}+/-{examEval.examScoreStd:0.2f} \t  n-exam={examEval.nExamScore:0.2f}')

examEvaluationPerQuery:Dict[str,float] = resultsPerMethod['my_method'].examCoverPerQuery

# Export Exam Cover Evaluation Scores
write_exam_results("exam-eval.jsonl.gz", resultsPerMethod)
```

