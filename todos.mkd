# EXAM Reproduction


1. [x] convert CAR Y3 submitted rankings and judgments to JSON 
    * done in haskell
    * file patterns "qrels-runs-and-text.jsonl.gz" and "qrels-and-text.jsonl.gz"
    * on /mnt/cherries

2. [x] FLAN-T5-large (out-of-the-box) QA system
    * using huggingface
3. [x] load TQA questions, noodle through QA system to get exam grades for all CAR Y3 submitted paragraphs
    * file pattern "exam-qrels...json.gz"
    * on peanut @dietz home
4. [x] from exam annotated paragraphs, perform kendall's tau with manual judgments
    * low performance on paragraph level  (tau = 0.05)
5. [ ] from exam annnotated paragraphs, export as exam.QRELS    
    * [x] code written,
    * [ ] run to produce qrels file
    * [ ] evaluate run files with `trec-eval` **TODO: evaluate runs with exam-qrels** 
6. [x] from exam annotated rankings, produce an exam-coverage score for each system. Produce a leaderboard.
    * [x] code for exam-coverage score (norm and plain)
    * [x] produce leaderboard of systems 
    * [ ] **TODO: write leaderboard as file**
    * [ ] **NEEDS to be redone when new `exam-qrels-runs...jsonl.gz` is finished**
7. [x] determine rank-correlation with official TREC leaderboard
    * [x] code for computing rank-correlation with `scipy` (handling ties!)
    * [x] kendall and spearman
    * [x] analysis: good correlation of spearman 0.84 and kendall 0.67 of plain exam and TREC
    * [ ] **todo: rank correlation of using exam-qrels file?**
    * [ ] **NEEDS to be redone when new `exam-qrels-runs...jsonl.gz` is finished**
8. [ ] need better QA system. Options:
    * FLAN-T5-large finetunes on squad2
    * Mixtral
9. [ ] evaluate exam of TQA gold article
10. [ ] Break code into re-usable component with cmd parser
11. [ ] Integrate in other frameworks
    * [ ] DSPy
    * [ ] LLM360 
12. [ ] Use to evaluate GPT-generated content
    * [ ] use the recorded files with Davinci-003
    * [ ] use gpt 4.0
20. [ ] merge with Naghmeh's chatGPT generated questions
30. [ ] different dataset?    