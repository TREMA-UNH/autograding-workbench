# EXAM Reproduction


1. [x] convert CAR Y3 submitted rankings and judgments to JSON 
    * done in haskell
    * file patterns "qrels-runs-and-text.jsonl.gz" and "qrels-and-text.jsonl.gz"
    * on /mnt/cherries
2. [x] FLAN-T5-large (out-of-the-box) QA system
    * using huggingface
3. [x] load TQA questions, noodle through QA system to get exam grades for all CAR Y3 submitted paragraphs
    * file pattern "exam-qrels...json.gz"
    * on peanut @dietz home
4. [x] from exam annotated paragraphs, perform kendall's tau with manual judgments
    * low performance on paragraph level  (tau = 0.05)
5. [ ] from exam annnotated paragraphs, export as exam.QRELS    
    * [x] code written,
    * [x] run to produce qrels file
    * [ ] evaluate run files with `trec-eval` **TODO: evaluate runs with exam-qrels** 
6. [x] from exam annotated rankings, produce an exam-coverage score for each system. Produce a leaderboard.
    * [x] code for exam-coverage score (norm and plain)
    * [x] produce leaderboard of systems 
    * [x] **TODO: write leaderboard as file**
    * [x] **NEEDS to be redone when new `exam-qrels-runs...jsonl.gz` is finished**
7. [x] determine rank-correlation with official TREC leaderboard
    * [x] code for computing rank-correlation with `scipy` (handling ties!)
    * [x] kendall and spearman
    * [x] analysis: good correlation of spearman 0.91 and kendall 0.77 of n-exam and TREC
    * [ ] **todo: rank correlation of using exam-qrels file?**
    * [x] **NEEDS to be redone when new `exam-qrels-runs...jsonl.gz` is finished**
8. [ ] on-the-fly exam score computation of given text.
    * [ ] needs slightly different format, break text into paragraphs (MD5-sum ids?), one file per system
    * [ ] run QA, and (optionally) directly evaluate EXAM
    * [ ] new output format. Per paragrah annotations of answerability. additional query and / system-level EXAM scores
10. [x] Break code into re-usable components with cmd parser
11. [ ] evaluate exam of TQA gold article
12. [ ] Use to evaluate GPT-generated content
    * [ ] use the recorded files with Davinci-003
    * [ ] use gpt 4.0
15. [ ] need better QA system. Options:
    * [x] FLAN-T5-large finetuned on squad2  (prepared code)
        * [x]  rerun analysis
    * [x] gpt2-large
    * [ ] Mixtral
16. [ ] Integrate in other frameworks
    * [ ] DSPy
    * [ ] LLM360 
17. [ ] Web-based leaderboard system
    * [ ] keep track of system runs, present as EXAM table
    * [ ] upload new runs, to produce new scores
    * [ ] add new exam questions, recompute all EXAM scores
20. [ ] merge with Naghmeh's chatGPT generated questions
30. [ ] different dataset?