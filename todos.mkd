# EXAM Reproduction


1. [x] convert CAR Y3 submitted rankings and judgments to JSON 
    * done in haskell
    * file patterns "qrels-runs-and-text.jsonl.gz" and "qrels-and-text.jsonl.gz"
    * on /mnt/cherries
2. [x] FLAN-T5-large (out-of-the-box) QA system
    * using huggingface
3. [x] load TQA questions, noodle through QA system to get exam grades for all CAR Y3 submitted paragraphs
    * file pattern "exam-qrels...json.gz"
    * on peanut @dietz home
3. [X] refactoring and robust infrastructure    
    * [X] refactored grading pipeline to support different prompts and answer checkers.
    * [X] grading pipeline supports restarts 
4. [x] from exam annotated paragraphs, perform kendall's tau with manual judgments
    * [x] low performance on paragraph level  (tau = 0.05)
    * [x] better performance with self-rating prompt (tau = 0.3)
    * [x] advanced correlation pipeline for ratings, judgments, and min_answers (produces latex tables)
5. [ ] from exam annnotated paragraphs, export as exam.QRELS    
    * [x] code written,
    * [x] run to produce qrels file
    * [ ] evaluate run files with `trec-eval` **TODO: evaluate runs with exam-qrels** 
6. [x] from exam annotated rankings, produce an exam-coverage score for each system. Produce a leaderboard.
    * [x] code for exam-coverage score (norm and plain)
    * [x] produce leaderboard of systems 
    * [x] **TODO: write leaderboard as file**
    * [x] **NEEDS to be redone when new `exam-qrels-runs...jsonl.gz` is finished**
7. [x] determine rank-correlation with official TREC leaderboard
    * [x] code for computing rank-correlation with `scipy` (handling ties!)
    * [x] kendall and spearman
    * [x] analysis: good correlation of spearman 0.91 and kendall 0.77 of n-exam and TREC
    * [ ] **todo: rank correlation of using exam-qrels file?**
    * [x] **NEEDS to be redone when new `exam-qrels-runs...jsonl.gz` is finished**
8. [ ] on-the-fly exam score computation of given text.
    * [ ] needs slightly different format, break text into paragraphs (MD5-sum ids?), one file per system
    * [ ] run QA, and (optionally) directly evaluate EXAM
    * [ ] new output format. Per paragrah annotations of answerability. additional query and / system-level EXAM scores
10. [x] Break code into re-usable components with cmd parser
11. [ ] evaluate exam of TQA gold article
12. [.] Use to evaluate GPT-generated content
    * [X] use the recorded files with Davinci-003
    * [ ] use gpt 4.0
    * [ ] evaluate
15. [ ] need better QA system. Options:
    * [x] FLAN-T5-large finetuned on squad2  (prepared code)
        * [x]  rerun analysis
    * [x] gpt2-large
    * [ ] Mixtral
    * [x] self-rated prompt on FLAN-T5-large
16. [ ] Integrate in other frameworks
    * [ ] DSPy
    * [ ] LLM360 
17. [ ] Web-based leaderboard system
    * [ ] keep track of system runs, present as EXAM table
    * [ ] upload new runs, to produce new scores
    * [ ] add new exam questions, recompute all EXAM scores
20. [x] merge with Naghmeh's chatGPT generated questions
    * [.] annotate car-y3 with Naghmeh's questions and self-rated prompt
30. [ ] different dataset?


# Analysis

## benchmarkY3 qrels (only)
1. [x] self-rating-prompt, tqa questions on benchY3 qrels. 
    * The data is available here: `/mnt/cherries/datasets/trec-car-data/additional/t5-cc-rating-tqa-exam--benchmarkY3test-qrels-with-text.jsonl.gz`
    * [x]  rating-vs-graded correlation analysis  (see mattermost channels and overleaf)
    * [x]  cc-anser-checked correlation analysis (see mattermost channels and overleaf)
    * [ ]  manual verification?


## benchmarkY3 runs and qrels
1. [ ] `t5-rating-naghmehs-exam-qrel-runs-result.json.gz` self-rating prompts, naghmeh's questions on carY3 runs&qrels paragraphs
    * [x] grading finished
2. [.] self-rating prompts, tqa questions, on CARy3 runs&qrels paragraphs  (running in tmux 0, GPU1)


## Davinci runs
1. [ ] self-rating prompts, tqa questions, on davinci runs
    * [x] grading finished (exampp2)
2. [ ] cc-answer prompts, tqa questions, on davinci runs
    * [X] grading 
3. [ ] self-rating prompts, naghmeh questions, on davinci runs
    * [.] grading runs (tmux 3, GPU0, exampp2)
4. [ ] cc prompts, naghmeh questions, on davinci runs
    * [ ] grading
5. [ ] qa prompts, tqa questions, on davinci runs
    * [ ] grading r
